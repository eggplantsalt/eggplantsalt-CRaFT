# Hidden Retention Loss å®ç°æ€»ç»“

## ä»»åŠ¡å®Œæˆæƒ…å†µ

âœ… **æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆå¹¶é€šè¿‡æµ‹è¯•**

---

## ğŸ“ ä¿®æ”¹æ–‡ä»¶åˆ—è¡¨

### ä¿®æ”¹æ–‡ä»¶ï¼ˆ1ä¸ªï¼‰

1. **src/lerobot/craft/retention_loss.py**
   - æ–°å¢ `compute_hidden_retention_loss(policy, anchor_batch, craft_config)` ä¸»å…¥å£å‡½æ•°
   - æ–°å¢ `extract_student_hidden_features()` æå– student hidden states
   - æ–°å¢ `pool_hidden_states()` æ”¯æŒ 4 ç§ pooling ç­–ç•¥
   - æ–°å¢ `identify_image_tokens()` è¯†åˆ«å›¾åƒ tokensï¼ˆ3 å±‚ fallbackï¼‰
   - æ›´æ–° `compute_retention_loss_hidden()` åœ¨ float32 ä¸­è®¡ç®—ï¼ˆç¨³å®šæ€§ï¼‰

### æ–°å¢æ–‡ä»¶ï¼ˆ1ä¸ªï¼‰

2. **tests/test_hidden_retention_loss_math.py** (340+ è¡Œ)
   - 7 ä¸ª CPU å•å…ƒæµ‹è¯•
   - ä½¿ç”¨ tiny mock Transformer éªŒè¯æ•°å­¦æ­£ç¡®æ€§
   - **æµ‹è¯•ç»“æœï¼šå…¨éƒ¨é€šè¿‡ âœ“**

---

## ğŸ¯ æ ¸å¿ƒåŠŸèƒ½å®ç°

### 1. compute_hidden_retention_loss()

ä¸»å…¥å£å‡½æ•°ï¼Œè´Ÿè´£å®Œæ•´çš„ retention loss è®¡ç®—æµç¨‹ã€‚

```python
def compute_hidden_retention_loss(
    policy,
    anchor_batch: dict,
    craft_config,
) -> tuple[Tensor, dict]:
    """
    è®¡ç®— hidden state ä¿ç•™æŸå¤±
    
    ã€è¿”å›å€¼ã€‘
    - loss: ä¿ç•™æŸå¤±æ ‡é‡å¼ é‡ï¼ˆfloat32ï¼‰
    - metrics: æŒ‡æ ‡å­—å…¸
        - retention_loss: loss å€¼
        - student_hidden_norm: student hidden states çš„èŒƒæ•°
        - target_features_norm: target features çš„èŒƒæ•°
    """
```

**å®ç°è¦ç‚¹ï¼š**
- âœ… ä½¿ç”¨ policy åŸç”Ÿçš„ `output_hidden_states=True`ï¼ˆæœ€å°ä¾µå…¥ï¼‰
- âœ… ä» `craft_config.hidden_layer` æå–æŒ‡å®šå±‚
- âœ… ä½¿ç”¨ä¸ cache ä¸€è‡´çš„ pooling ç­–ç•¥
- âœ… åœ¨ float32 ä¸­è®¡ç®— lossï¼ˆæ•°å€¼ç¨³å®šï¼‰
- âœ… æ”¯æŒåå‘ä¼ æ’­

### 2. extract_student_hidden_features()

ä» student æ¨¡å‹æå– hidden featuresã€‚

```python
def extract_student_hidden_features(
    policy,
    anchor_batch: dict,
    craft_config,
) -> Tensor:
    """
    æå– student hidden features
    
    ã€è¿”å›å€¼ã€‘
    Tensor: [B, hidden_dim] çš„ pooled features
    """
```

**å®ç°ç­–ç•¥ï¼š**
1. **ä¼˜å…ˆä½¿ç”¨åŸç”Ÿ API**ï¼š
   ```python
   if hasattr(policy, '_paligemma_model'):
       outputs = policy._paligemma_model(
           pixel_values=pixel_values,
           input_ids=input_ids,
           attention_mask=attention_mask,
           output_hidden_states=True,
       )
   ```

2. **Fallback æ‰‹åŠ¨æ„é€ **ï¼š
   ```python
   else:
       # æ‰‹åŠ¨æ„é€  forward pass
       prefix_embs, ... = policy.model.embed_prefix_fast(...)
       outputs = language_model.forward(
           inputs_embeds=prefix_embs,
           output_hidden_states=True,
       )
   ```

3. **æå–æŒ‡å®šå±‚**ï¼š
   ```python
   hidden_layer = meta.get("hidden_layer", -2)
   if hidden_layer < 0:
       actual_idx = total_layers + hidden_layer + 1
   hidden_state = all_hidden_states[actual_idx]
   ```

### 3. pool_hidden_states()

æ”¯æŒ 4 ç§ pooling ç­–ç•¥ã€‚

```python
def pool_hidden_states(
    hidden_states: Tensor,  # [B, seq_len, hidden_dim]
    attention_mask: Tensor,
    pooling: str,
    policy,
    input_ids: Tensor,
) -> Tensor:  # [B, hidden_dim]
```

**Pooling ç­–ç•¥ï¼š**

#### mean_image_tokensï¼ˆæ¨èï¼‰
```python
# è¯†åˆ«å›¾åƒ tokens èŒƒå›´
num_image_tokens = identify_image_tokens(policy)  # 196 for PaliGemma

# æå–å›¾åƒ tokens
image_hidden = hidden_states[:, :num_image_tokens, :]  # [B, 196, hidden_dim]

# å¹³å‡æ± åŒ–
pooled = image_hidden.mean(dim=1)  # [B, hidden_dim]
```

#### mean_masked
```python
# å¯¹æ‰€æœ‰é padding tokens å–å¹³å‡
mask = attention_mask.unsqueeze(-1).float()  # [B, seq_len, 1]
masked_hidden = hidden_states * mask
pooled = masked_hidden.sum(dim=1) / (mask.sum(dim=1) + 1e-9)
```

#### last_token
```python
# å–æœ€åä¸€ä¸ªé padding token
lengths = attention_mask.sum(dim=1).long() - 1  # [B]
pooled = hidden_states[torch.arange(B), lengths]
```

#### cls_token
```python
# å–ç¬¬ä¸€ä¸ª token
pooled = hidden_states[:, 0, :]
```

### 4. identify_image_tokens()

è¯†åˆ«å›¾åƒ tokens æ•°é‡ï¼ˆ3 å±‚ fallbackï¼‰ã€‚

```python
def identify_image_tokens(policy) -> int:
    # æ–¹æ³• 1: ä» policy config è·å–ï¼ˆä¼˜å…ˆï¼‰
    if hasattr(policy, 'config') and hasattr(policy.config, 'image_seq_length'):
        return policy.config.image_seq_length
    
    # æ–¹æ³• 2: è®¡ç®—
    if hasattr(policy, 'config') and hasattr(policy.config, 'image_resolution'):
        h, w = policy.config.image_resolution
        patch_size = policy.config.patch_size
        return (h // patch_size) * (w // patch_size)
    
    # æ–¹æ³• 3: é»˜è®¤å€¼ï¼ˆPaliGemma 224x224, patch_size=16ï¼‰
    return 196
```

---

## âœ… æµ‹è¯•ç»“æœ

### 7 ä¸ªå•å…ƒæµ‹è¯•å…¨éƒ¨é€šè¿‡

```
============================================================
[SUCCESS] All tests passed!
============================================================

Test 1: MSE Loss Correctness
[OK] MSE Loss: 1.979173
[OK] Expected: 1.979173
[OK] Difference: 0.00e+00

Test 2: Cosine Loss Correctness
[OK] Cosine Loss: 1.012244
[OK] Expected: 1.012244
[OK] Cosine Similarity: -0.012243

Test 3: Loss Range
[OK] Identical hidden states:
     MSE Loss: 0.00e+00 (should be ~0)
     Cosine Loss: -4.47e-08 (should be ~0)
[OK] Opposite hidden states:
     Cosine Loss: 2.000000 (should be ~2)

Test 4: Gradient Flow
[OK] Gradient exists: True
[OK] Gradient norm: 0.086421
[OK] Gradient shape: torch.Size([4, 2, 2, 64])

Test 5: Pooling Strategies
[OK] mean_image_tokens   : shape=torch.Size([4, 64]), norm=1.8710
[OK] mean_masked         : shape=torch.Size([4, 64]), norm=1.1177
[OK] last_token          : shape=torch.Size([4, 64]), norm=8.1868
[OK] cls_token           : shape=torch.Size([4, 64]), norm=8.2046

Test 6: Float32 Stability
[OK] Input dtype: torch.float16
[OK] Loss dtype: torch.float32
[OK] Loss value: 1.916182
[OK] Loss is finite: True

Test 7: End-to-End with Tiny Transformer
[OK] Student hidden shape: torch.Size([4, 20, 64])
[OK] Teacher hidden shape: torch.Size([4, 20, 64])
[OK] Pooled shape: torch.Size([4, 64])
[OK] Loss: 0.268769
[OK] Gradients exist: True
```

---

## ğŸ”§ ä½¿ç”¨ç¤ºä¾‹

### åœ¨è®­ç»ƒå¾ªç¯ä¸­ä½¿ç”¨

```python
from lerobot.craft.retention_loss import compute_hidden_retention_loss

# åœ¨è®­ç»ƒå¾ªç¯ä¸­
for step in range(total_steps):
    # 1. è®¡ç®—ä»»åŠ¡æŸå¤±
    task_batch = next(task_dataloader)
    task_loss, _ = policy.forward(task_batch)
    
    # 2. è®¡ç®—ä¿ç•™æŸå¤±ï¼ˆæ¯ K æ­¥ï¼‰
    if step % craft_config.retention_freq == 0:
        anchor_batch = next(anchor_dl_iter)
        
        # è®¡ç®— hidden retention loss
        retention_loss, metrics = compute_hidden_retention_loss(
            policy,
            anchor_batch,
            craft_config
        )
        
        # è®°å½• metrics
        print(f"Retention Loss: {metrics['retention_loss']:.4f}")
        print(f"Student Norm: {metrics['student_hidden_norm']:.4f}")
        print(f"Target Norm: {metrics['target_features_norm']:.4f}")
        
        # åå‘ä¼ æ’­
        retention_loss.backward()
```

### é…ç½®ç¤ºä¾‹

```python
from lerobot.craft import CraftConfig

craft_config = CraftConfig(
    enabled=True,
    anchor_cache_dir="data/anchor_hidden_cache",
    hidden_layer=-2,  # å€’æ•°ç¬¬äºŒå±‚
    pooling="mean_image_tokens",  # æ¨è
    loss_type="mse",  # æˆ– "cosine"
    retention_freq=1,  # æ¯æ­¥è®¡ç®—
)
```

---

## ğŸ“Š å…³é”®è®¾è®¡å†³ç­–

### 1. ä¸ºä»€ä¹ˆä½¿ç”¨åŸç”Ÿ output_hidden_statesï¼Ÿ

**ä¼˜ç‚¹ï¼š**
- âœ… æœ€å°ä¾µå…¥ï¼šä¸ä¿®æ”¹æ¨¡å‹ç»“æ„
- âœ… å…¼å®¹æ€§å¥½ï¼šå¤§å¤šæ•° Transformer æ¨¡å‹éƒ½æ”¯æŒ
- âœ… æ€§èƒ½ä¼˜åŒ–ï¼šæ¨¡å‹å†…éƒ¨å·²ä¼˜åŒ–
- âœ… æ˜“äºç»´æŠ¤ï¼šä¸ä¾èµ– forward hook

**å®ç°ï¼š**
```python
# ä¼˜å…ˆä½¿ç”¨åŸç”Ÿ API
outputs = policy._paligemma_model(
    pixel_values=pixel_values,
    input_ids=input_ids,
    attention_mask=attention_mask,
    output_hidden_states=True,  # å…³é”®å‚æ•°
)
all_hidden_states = outputs.hidden_states
```

### 2. ä¸ºä»€ä¹ˆåœ¨ float32 ä¸­è®¡ç®— lossï¼Ÿ

**åŸå› ï¼š**
- âœ… æ•°å€¼ç¨³å®šï¼šé¿å… float16 çš„ç²¾åº¦é—®é¢˜
- âœ… æ¢¯åº¦ç¨³å®šï¼šfloat32 æ¢¯åº¦æ›´å‡†ç¡®
- âœ… å…¼å®¹æ€§ï¼šPyTorch ä¼˜åŒ–å™¨é€šå¸¸ä½¿ç”¨ float32

**å®ç°ï¼š**
```python
# è½¬æ¢åˆ° float32
student_features_f32 = student_features.float()
target_features_f32 = target_features.float()

# è®¡ç®— lossï¼ˆfloat32ï¼‰
loss = F.mse_loss(student_features_f32, target_features_f32)
```

### 3. ä¸ºä»€ä¹ˆæ¨è mean_image_tokens poolingï¼Ÿ

**ä¼˜ç‚¹ï¼š**
- âœ… è¯­ä¹‰ä¸°å¯Œï¼šå›¾åƒ tokens åŒ…å«è§†è§‰ä¿¡æ¯
- âœ… ç¨³å®šæ€§å¥½ï¼šå¹³å‡æ± åŒ–æ¯”å•ä¸ª token æ›´é²æ£’
- âœ… ä»»åŠ¡ç›¸å…³ï¼šæœºå™¨äººä»»åŠ¡ä¸»è¦ä¾èµ–è§†è§‰è¾“å…¥
- âœ… å®éªŒéªŒè¯ï¼šåœ¨ vision-language æ¨¡å‹ä¸­è¡¨ç°è‰¯å¥½

**å¯¹æ¯”ï¼š**
| Pooling | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |
|---------|------|------|---------|
| mean_image_tokens | è¯­ä¹‰ä¸°å¯Œã€ç¨³å®š | éœ€è¦è¯†åˆ«å›¾åƒ tokens | è§†è§‰ä»»åŠ¡ï¼ˆæ¨èï¼‰ |
| mean_masked | ç®€å•ã€é€šç”¨ | åŒ…å«æ–‡æœ¬ä¿¡æ¯ | é€šç”¨åœºæ™¯ |
| last_token | æ•è·åºåˆ—ä¿¡æ¯ | å•ç‚¹ä¸ç¨³å®š | åºåˆ—ä»»åŠ¡ |
| cls_token | å…¨å±€è¡¨å¾ | ä¾èµ–æ¨¡å‹è®¾è®¡ | åˆ†ç±»ä»»åŠ¡ |

### 4. ä¸ºä»€ä¹ˆæ”¯æŒ MSE å’Œ Cosine lossï¼Ÿ

**MSE Lossï¼ˆæ¨èï¼‰ï¼š**
```python
loss = F.mse_loss(student_features, target_features)
```
- âœ… ç›´æ¥ä¼˜åŒ–ï¼šæœ€å°åŒ–ç‰¹å¾å·®å¼‚
- âœ… ç¨³å®šæ€§å¥½ï¼šæ¢¯åº¦å¹³æ»‘
- âœ… æ˜“äºè°ƒè¯•ï¼šloss å€¼ç›´è§‚

**Cosine Lossï¼š**
```python
cosine_sim = F.cosine_similarity(student_features, target_features, dim=1)
loss = (1.0 - cosine_sim).mean()
```
- âœ… æ–¹å‘å¯¹é½ï¼šå…³æ³¨ç‰¹å¾æ–¹å‘è€Œéå¹…åº¦
- âœ… å½’ä¸€åŒ–ï¼šå¯¹ç‰¹å¾å°ºåº¦ä¸æ•æ„Ÿ
- âœ… é€‚ç”¨åœºæ™¯ï¼šå½“ç‰¹å¾å¹…åº¦å˜åŒ–å¤§æ—¶

---

## ğŸ“ Git Commit

```
commit: 4684ec00
message: feat: hidden-state retention loss implementation + tests

Files changed:
- src/lerobot/craft/retention_loss.py (ä¿®æ”¹)
- tests/test_hidden_retention_loss_math.py (æ–°å¢)

Key features:
- Uses model native output_hidden_states=True (minimal intrusion)
- Extracts hidden states from configurable layer (default -2)
- Supports 4 pooling strategies: mean_image_tokens, mean_masked, last_token, cls_token
- Computes loss in float32 for numerical stability
- Supports MSE and cosine loss
- Full gradient flow verified

All tests passed (7/7)

Status: âœ“ Committed, not pushed
```

---

## ğŸš€ ä¸‹ä¸€æ­¥

1. **é›†æˆåˆ°è®­ç»ƒå¾ªç¯**
   - åœ¨ `lerobot_train_craft.py` ä¸­è°ƒç”¨ `compute_hidden_retention_loss()`
   - æ›¿æ¢ç°æœ‰çš„ `compute_retention_loss_hidden()`

2. **ç«¯åˆ°ç«¯æµ‹è¯•**
   - åœ¨çœŸå®æ•°æ®é›†ä¸Šæµ‹è¯•
   - éªŒè¯æ¢¯åº¦æµå’Œ loss æ”¶æ•›

3. **æ€§èƒ½å¯¹æ¯”**
   - å¯¹æ¯” MSE vs Cosine loss
   - å¯¹æ¯”ä¸åŒ pooling ç­–ç•¥
   - å¯¹æ¯”ä¸åŒ hidden_layer é€‰æ‹©

4. **æ–‡æ¡£æ›´æ–°**
   - æ›´æ–° CRaFT è®­ç»ƒæŒ‡å—
   - æ·»åŠ  retention_mode=hidden ä½¿ç”¨æ•™ç¨‹

