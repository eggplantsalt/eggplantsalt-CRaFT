# API å‚è€ƒæ–‡æ¡£

> CRaFT æ¨¡å—å®Œæ•´ API æ–‡æ¡£

---

## ğŸ“‹ ç›®å½•

1. [CraftConfig](#craftconfig)
2. [æ¢¯åº¦æ‰‹æœ¯æ¨¡å—](#æ¢¯åº¦æ‰‹æœ¯æ¨¡å—)
3. [åŸå¯¹å¶ä¼˜åŒ–](#åŸå¯¹å¶ä¼˜åŒ–)
4. [ä¿ç•™æŸå¤±](#ä¿ç•™æŸå¤±)
5. [é”šç‚¹æ•°æ®åŠ è½½](#é”šç‚¹æ•°æ®åŠ è½½)
6. [è®­ç»ƒè„šæœ¬](#è®­ç»ƒè„šæœ¬)

---

## CraftConfig

### ç±»å®šä¹‰

```python
from lerobot.craft import CraftConfig

@dataclass
class CraftConfig:
    """CRaFT è®­ç»ƒé…ç½®ç±»"""
    
    # å¯ç”¨/ç¦ç”¨
    enabled: bool = False
    
    # é”šç‚¹æ•°æ®é›†é…ç½®
    anchor_cache_dir: str = ""
    anchor_batch_size: int = 16
    retention_freq: int = 5
    retention_mode: str = "hidden"  # "token_ce" æˆ– "hidden"
    
    # æŸå¤±æƒé‡ï¼ˆåŸå¯¹å¶ä¼˜åŒ–ï¼‰
    initial_lambda: float = 1.0
    lambda_lr: float = 0.01
    lambda_max: float = 10.0
    
    # ä¿ç•™çº¦æŸï¼ˆÎµ è°ƒåº¦ï¼‰
    epsilon_start: float = 1.0
    epsilon_end: float = 0.1
    epsilon_decay_steps: int = 0  # 0 è¡¨ç¤ºä½¿ç”¨ training steps
    
    # æ¢¯åº¦æ‰‹æœ¯
    use_grad_projection: bool = True
    conflict_threshold: float = -0.1
    projection_mode: str = "weighted"  # "weighted", "equal", "task_priority"
    
    # æ—¥å¿—è®°å½•
    log_craft_metrics_freq: int = 100
    save_lambda_history: bool = True
```

### å‚æ•°è¯´æ˜

#### enabled
- **ç±»å‹**: `bool`
- **é»˜è®¤å€¼**: `False`
- **è¯´æ˜**: æ˜¯å¦å¯ç”¨ CRaFT è®­ç»ƒ
- **ç¤ºä¾‹**:
  ```python
  config = CraftConfig(enabled=True)
  ```

#### anchor_cache_dir
- **ç±»å‹**: `str`
- **é»˜è®¤å€¼**: `""`
- **è¯´æ˜**: AnchorCache ç›®å½•è·¯å¾„
- **è¦æ±‚**: å¦‚æœ `enabled=True`ï¼Œå¿…é¡»æä¾›
- **ç¤ºä¾‹**:
  ```python
  config = CraftConfig(
      enabled=True,
      anchor_cache_dir="data/anchor_hidden_cache"
  )
  ```

#### retention_mode
- **ç±»å‹**: `str`
- **é»˜è®¤å€¼**: `"hidden"`
- **å¯é€‰å€¼**: `"token_ce"`, `"hidden"`
- **è¯´æ˜**: ä¿ç•™æŸå¤±è®¡ç®—æ¨¡å¼
  - `"token_ce"`: Token-level cross-entropy loss
  - `"hidden"`: Hidden state retention lossï¼ˆæ¨èï¼‰
- **ç¤ºä¾‹**:
  ```python
  config = CraftConfig(retention_mode="hidden")
  ```

#### initial_lambda
- **ç±»å‹**: `float`
- **é»˜è®¤å€¼**: `1.0`
- **èŒƒå›´**: `[0.0, inf)`
- **è¯´æ˜**: Lagrangian ä¹˜å­ Î» çš„åˆå§‹å€¼
- **å»ºè®®**: ä» 1.0 å¼€å§‹ï¼Œæ ¹æ®å®éªŒè°ƒæ•´
- **ç¤ºä¾‹**:
  ```python
  config = CraftConfig(initial_lambda=2.0)
  ```

#### epsilon_start / epsilon_end
- **ç±»å‹**: `float`
- **é»˜è®¤å€¼**: `1.0` / `0.1`
- **è¯´æ˜**: ä¿ç•™æŸå¤±é˜ˆå€¼çš„èµ·å§‹/ç»“æŸå€¼
- **é€€ç«**: ä» `epsilon_start` çº¿æ€§é€€ç«åˆ° `epsilon_end`
- **ç¤ºä¾‹**:
  ```python
  config = CraftConfig(
      epsilon_start=1.5,
      epsilon_end=0.05
  )
  ```

#### use_grad_projection
- **ç±»å‹**: `bool`
- **é»˜è®¤å€¼**: `True`
- **è¯´æ˜**: æ˜¯å¦å¯ç”¨æ¢¯åº¦æŠ•å½±ï¼ˆè§£å†³æ¢¯åº¦å†²çªï¼‰
- **ç®—æ³•**: åŸºäº PCGrad
- **ç¤ºä¾‹**:
  ```python
  config = CraftConfig(use_grad_projection=True)
  ```

---

## æ¢¯åº¦æ‰‹æœ¯æ¨¡å—

### compute_dot

è®¡ç®—ä¸¤ä¸ªæ¢¯åº¦å‘é‡çš„ç‚¹ç§¯ã€‚

```python
from lerobot.craft.grad_surgery import compute_dot

def compute_dot(
    grads_a: list[torch.Tensor | None],
    grads_b: list[torch.Tensor | None]
) -> torch.Tensor:
    """
    è®¡ç®—ä¸¤ä¸ªæ¢¯åº¦å‘é‡çš„ç‚¹ç§¯
    
    å‚æ•°:
        grads_a: ç¬¬ä¸€ä¸ªæ¢¯åº¦åˆ—è¡¨
        grads_b: ç¬¬äºŒä¸ªæ¢¯åº¦åˆ—è¡¨
    
    è¿”å›:
        dot_product: ç‚¹ç§¯æ ‡é‡å¼ é‡
    
    ç¤ºä¾‹:
        >>> task_grads = [p.grad for p in model.parameters()]
        >>> ret_grads = [p.grad for p in model.parameters()]
        >>> dot = compute_dot(task_grads, ret_grads)
        >>> print(f"Dot product: {dot.item()}")
    """
```

**æ•°å­¦å®šä¹‰**:
```
dot = Î£ (g_a[i] Â· g_b[i])
```

### project_if_conflict

æ£€æµ‹æ¢¯åº¦å†²çªå¹¶è¿›è¡ŒæŠ•å½±ã€‚

```python
from lerobot.craft.grad_surgery import project_if_conflict

def project_if_conflict(
    grads_task: list[torch.Tensor | None],
    grads_retain: list[torch.Tensor | None],
    conflict_threshold: float = -0.1
) -> tuple[list[torch.Tensor | None], list[torch.Tensor | None], bool]:
    """
    æ£€æµ‹æ¢¯åº¦å†²çªå¹¶è¿›è¡ŒæŠ•å½±
    
    å‚æ•°:
        grads_task: ä»»åŠ¡æ¢¯åº¦
        grads_retain: ä¿ç•™æ¢¯åº¦
        conflict_threshold: å†²çªé˜ˆå€¼ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
    
    è¿”å›:
        grads_task_proj: æŠ•å½±åçš„ä»»åŠ¡æ¢¯åº¦
        grads_retain_proj: æŠ•å½±åçš„ä¿ç•™æ¢¯åº¦
        conflict_detected: æ˜¯å¦æ£€æµ‹åˆ°å†²çª
    
    ç¤ºä¾‹:
        >>> task_proj, ret_proj, conflict = project_if_conflict(
        ...     task_grads, ret_grads, conflict_threshold=-0.1
        ... )
        >>> if conflict:
        ...     print("Gradient conflict detected and resolved!")
    """
```

**ç®—æ³•**:
```
å¦‚æœ cos(g_task, g_retain) < threshold:
    g_task_proj = g_task - (g_task Â· g_retain / ||g_retain||Â²) * g_retain
    g_retain_proj = g_retain - (g_retain Â· g_task / ||g_task||Â²) * g_task
å¦åˆ™:
    g_task_proj = g_task
    g_retain_proj = g_retain
```

### merge_grads

åˆå¹¶ä»»åŠ¡æ¢¯åº¦å’Œä¿ç•™æ¢¯åº¦ã€‚

```python
from lerobot.craft.grad_surgery import merge_grads

def merge_grads(
    grads_task: list[torch.Tensor | None],
    grads_retain: list[torch.Tensor | None],
    lambda_weight: float,
    mode: str = "weighted"
) -> list[torch.Tensor | None]:
    """
    åˆå¹¶ä»»åŠ¡æ¢¯åº¦å’Œä¿ç•™æ¢¯åº¦
    
    å‚æ•°:
        grads_task: ä»»åŠ¡æ¢¯åº¦
        grads_retain: ä¿ç•™æ¢¯åº¦
        lambda_weight: Î» æƒé‡
        mode: åˆå¹¶æ¨¡å¼
    
    è¿”å›:
        merged_grads: åˆå¹¶åçš„æ¢¯åº¦
    
    ç¤ºä¾‹:
        >>> final_grads = merge_grads(
        ...     task_grads, ret_grads, lambda_weight=1.5, mode="weighted"
        ... )
    """
```

**åˆå¹¶æ¨¡å¼**:
- `"weighted"`: `g_final = g_task + Î» * g_retain`
- `"equal"`: `g_final = 0.5 * (g_task + g_retain)`
- `"task_priority"`: `g_final = g_task + min(Î», 1.0) * g_retain`

---

## åŸå¯¹å¶ä¼˜åŒ–

### epsilon_schedule

è®¡ç®—å½“å‰æ­¥çš„ Îµ å€¼ã€‚

```python
from lerobot.craft.primal_dual import epsilon_schedule

def epsilon_schedule(
    step: int,
    epsilon_start: float,
    epsilon_end: float,
    total_steps: int,
    schedule_type: str = "linear"
) -> float:
    """
    è®¡ç®—å½“å‰æ­¥çš„ epsilon å€¼
    
    å‚æ•°:
        step: å½“å‰è®­ç»ƒæ­¥æ•°
        epsilon_start: èµ·å§‹å€¼
        epsilon_end: ç»“æŸå€¼
        total_steps: æ€»æ­¥æ•°
        schedule_type: è°ƒåº¦ç±»å‹
    
    è¿”å›:
        epsilon: å½“å‰ Îµ å€¼
    
    ç¤ºä¾‹:
        >>> eps = epsilon_schedule(
        ...     step=5000, epsilon_start=1.0, epsilon_end=0.1, total_steps=10000
        ... )
        >>> print(f"Current epsilon: {eps:.4f}")  # 0.5500
    """
```

**è°ƒåº¦ç±»å‹**:
- `"linear"`: çº¿æ€§é€€ç«
- `"cosine"`: ä½™å¼¦é€€ç«
- `"exponential"`: æŒ‡æ•°é€€ç«

### update_lambda

æ›´æ–° Lagrangian ä¹˜å­ Î»ã€‚

```python
from lerobot.craft.primal_dual import update_lambda

def update_lambda(
    current_lambda: float,
    retention_loss: float,
    epsilon: float,
    lambda_lr: float,
    lambda_max: float
) -> float:
    """
    æ›´æ–° Lagrangian ä¹˜å­ Î»
    
    å‚æ•°:
        current_lambda: å½“å‰ Î» å€¼
        retention_loss: ä¿ç•™æŸå¤±å€¼
        epsilon: å½“å‰ Îµ é˜ˆå€¼
        lambda_lr: Î» å­¦ä¹ ç‡
        lambda_max: Î» æœ€å¤§å€¼
    
    è¿”å›:
        new_lambda: æ›´æ–°åçš„ Î» å€¼
    
    ç¤ºä¾‹:
        >>> new_lambda = update_lambda(
        ...     current_lambda=1.0,
        ...     retention_loss=0.8,
        ...     epsilon=1.0,
        ...     lambda_lr=0.01,
        ...     lambda_max=10.0
        ... )
        >>> print(f"New lambda: {new_lambda:.4f}")  # 0.998
    """
```

**æ›´æ–°è§„åˆ™**:
```
Î»_new = clip(Î» + Î»_lr * (L_retain - Îµ), 0, Î»_max)
```

---

## ä¿ç•™æŸå¤±

### compute_hidden_retention_loss

è®¡ç®— hidden state ä¿ç•™æŸå¤±ï¼ˆä¸»å…¥å£ï¼‰ã€‚

```python
from lerobot.craft.retention_loss import compute_hidden_retention_loss

def compute_hidden_retention_loss(
    policy: PreTrainedPolicy,
    anchor_batch: dict,
    craft_config: CraftConfig
) -> tuple[torch.Tensor, dict]:
    """
    è®¡ç®— hidden state ä¿ç•™æŸå¤±
    
    å‚æ•°:
        policy: ç­–ç•¥æ¨¡å‹
        anchor_batch: é”šç‚¹æ•°æ®æ‰¹æ¬¡
        craft_config: CRaFT é…ç½®
    
    è¿”å›:
        loss: ä¿ç•™æŸå¤±å¼ é‡
        metrics: æŒ‡æ ‡å­—å…¸
    
    ç¤ºä¾‹:
        >>> anchor_batch = next(anchor_dl_iter)
        >>> loss, metrics = compute_hidden_retention_loss(
        ...     policy, anchor_batch, craft_config
        ... )
        >>> print(f"Retention loss: {metrics['retention_loss']:.4f}")
    """
```

**anchor_batch æ ¼å¼**:
```python
{
    "pixel_values": torch.Tensor,  # [B, C, H, W]
    "input_ids": torch.Tensor,     # [B, seq_len]
    "attention_mask": torch.Tensor,  # [B, seq_len]
    "target_features": torch.Tensor,  # [B, hidden_dim]
    "meta": {
        "hidden_layer": int,
        "pooling": str,
        "dtype": str
    }
}
```

### extract_student_hidden_features

æå– student æ¨¡å‹çš„ hidden featuresã€‚

```python
from lerobot.craft.retention_loss import extract_student_hidden_features

def extract_student_hidden_features(
    policy: PreTrainedPolicy,
    anchor_batch: dict,
    craft_config: CraftConfig
) -> torch.Tensor:
    """
    æå– student hidden features
    
    å‚æ•°:
        policy: ç­–ç•¥æ¨¡å‹
        anchor_batch: é”šç‚¹æ•°æ®
        craft_config: CRaFT é…ç½®
    
    è¿”å›:
        features: Hidden features [B, hidden_dim]
    
    ç¤ºä¾‹:
        >>> features = extract_student_hidden_features(
        ...     policy, anchor_batch, craft_config
        ... )
        >>> print(f"Feature shape: {features.shape}")  # [8, 2048]
    """
```

---

## é”šç‚¹æ•°æ®åŠ è½½

### AnchorCacheDataset

é”šç‚¹æ•°æ®é›†ç±»ã€‚

```python
from lerobot.craft.anchor_cache import AnchorCacheDataset

class AnchorCacheDataset(torch.utils.data.Dataset):
    """
    é”šç‚¹æ•°æ®é›†åŠ è½½å™¨
    
    å‚æ•°:
        cache_dir: Cache ç›®å½•è·¯å¾„
        transform: å¯é€‰çš„æ•°æ®è½¬æ¢
    
    ç¤ºä¾‹:
        >>> dataset = AnchorCacheDataset(
        ...     cache_dir="data/anchor_hidden_cache"
        ... )
        >>> print(f"Dataset size: {len(dataset)}")
        >>> sample = dataset[0]
        >>> print(f"Sample keys: {sample.keys()}")
    """
    
    def __init__(self, cache_dir: str, transform=None):
        pass
    
    def __len__(self) -> int:
        """è¿”å›æ•°æ®é›†å¤§å°"""
        pass
    
    def __getitem__(self, idx: int) -> dict:
        """è·å–å•ä¸ªæ ·æœ¬"""
        pass
```

**è¿”å›æ ¼å¼**:
```python
{
    "pixel_values": torch.Tensor,
    "input_ids": torch.Tensor,
    "attention_mask": torch.Tensor,
    "target_features": torch.Tensor,  # ä»… hidden mode
    "labels": torch.Tensor,  # ä»… token_ce mode
    "meta": dict
}
```

---

## è®­ç»ƒè„šæœ¬

### lerobot_train_craft

CRaFT è®­ç»ƒä¸»å‡½æ•°ã€‚

```python
from lerobot.scripts.lerobot_train_craft import train_craft

@parser.wrap()
def train_craft(
    cfg: TrainPipelineConfig,
    craft_config: CraftConfig | None = None,
    accelerator: Accelerator | None = None
):
    """
    CRaFT è®­ç»ƒä¸»å‡½æ•°
    
    å‚æ•°:
        cfg: è®­ç»ƒé…ç½®
        craft_config: CRaFT é…ç½®
        accelerator: åˆ†å¸ƒå¼è®­ç»ƒåŠ é€Ÿå™¨
    
    ç¤ºä¾‹:
        >>> from lerobot.configs.train import TrainPipelineConfig
        >>> from lerobot.craft import CraftConfig
        >>> 
        >>> cfg = TrainPipelineConfig(...)
        >>> craft_cfg = CraftConfig(enabled=True, ...)
        >>> 
        >>> train_craft(cfg, craft_config=craft_cfg)
    """
```

### update_policy_craft

å•æ­¥ CRaFT è®­ç»ƒæ›´æ–°ã€‚

```python
from lerobot.scripts.lerobot_train_craft import update_policy_craft

def update_policy_craft(
    train_metrics: MetricsTracker,
    policy: PreTrainedPolicy,
    task_batch: dict,
    anchor_batch: dict | None,
    optimizer: Optimizer,
    grad_clip_norm: float,
    accelerator: Accelerator,
    craft_config: CraftConfig,
    current_lambda: float,
    current_epsilon: float,
    lr_scheduler=None,
    lock=None
) -> tuple[MetricsTracker, dict, float]:
    """
    æ‰§è¡Œå•æ­¥ CRaFT è®­ç»ƒ
    
    å‚æ•°:
        train_metrics: è®­ç»ƒæŒ‡æ ‡è·Ÿè¸ªå™¨
        policy: ç­–ç•¥æ¨¡å‹
        task_batch: ä»»åŠ¡æ•°æ®æ‰¹æ¬¡
        anchor_batch: é”šç‚¹æ•°æ®æ‰¹æ¬¡
        optimizer: ä¼˜åŒ–å™¨
        grad_clip_norm: æ¢¯åº¦è£å‰ªèŒƒæ•°
        accelerator: åŠ é€Ÿå™¨
        craft_config: CRaFT é…ç½®
        current_lambda: å½“å‰ Î» å€¼
        current_epsilon: å½“å‰ Îµ å€¼
        lr_scheduler: å­¦ä¹ ç‡è°ƒåº¦å™¨
        lock: çº¿ç¨‹é”
    
    è¿”å›:
        train_metrics: æ›´æ–°åçš„æŒ‡æ ‡
        output_dict: è¾“å‡ºå­—å…¸
        new_lambda: æ›´æ–°åçš„ Î» å€¼
    
    è®­ç»ƒæµç¨‹:
        1. å‰å‘ä¼ æ’­ï¼ˆä»»åŠ¡æ•°æ®ï¼‰â†’ L_task
        2. åå‘ä¼ æ’­ â†’ âˆ‡L_task
        3. å‰å‘ä¼ æ’­ï¼ˆé”šç‚¹æ•°æ®ï¼‰â†’ L_retain
        4. åå‘ä¼ æ’­ â†’ âˆ‡L_retain
        5. æ¢¯åº¦æ‰‹æœ¯ï¼ˆæŠ•å½±ï¼‰
        6. åˆå¹¶æ¢¯åº¦
        7. ä¼˜åŒ–å™¨æ›´æ–°
        8. æ›´æ–° Î»
    """
```

---

## ä½¿ç”¨ç¤ºä¾‹

### å®Œæ•´è®­ç»ƒæµç¨‹

```python
from lerobot.configs.train import TrainPipelineConfig
from lerobot.craft import CraftConfig
from lerobot.scripts.lerobot_train_craft import train_craft

# 1. åˆ›å»ºé…ç½®
train_cfg = TrainPipelineConfig(
    policy=PolicyConfig(path="lerobot/pi0_fast"),
    dataset=DatasetConfig(repo_id="lerobot/aloha_sim_insertion_human"),
    training=TrainingConfig(
        offline_steps=10000,
        batch_size=8,
        lr=1e-4
    ),
    output_dir="outputs/craft_training"
)

craft_cfg = CraftConfig(
    enabled=True,
    retention_mode="hidden",
    anchor_cache_dir="data/anchor_hidden_cache",
    anchor_batch_size=8,
    retention_freq=1,
    initial_lambda=1.0,
    lambda_lr=0.01,
    epsilon_start=1.0,
    epsilon_end=0.1,
    use_grad_projection=True
)

# 2. è¿è¡Œè®­ç»ƒ
train_craft(train_cfg, craft_config=craft_cfg)
```

### è‡ªå®šä¹‰æ¢¯åº¦æ‰‹æœ¯

```python
from lerobot.craft.grad_surgery import compute_dot, project_if_conflict, merge_grads

# 1. è®¡ç®—ä»»åŠ¡æ¢¯åº¦
task_loss.backward()
task_grads = [p.grad.clone() for p in model.parameters()]

# 2. è®¡ç®—ä¿ç•™æ¢¯åº¦
optimizer.zero_grad()
retention_loss.backward()
retention_grads = [p.grad.clone() for p in model.parameters()]

# 3. æ£€æµ‹å†²çª
dot = compute_dot(task_grads, retention_grads)
print(f"Gradient dot product: {dot.item()}")

# 4. æŠ•å½±ï¼ˆå¦‚æœå†²çªï¼‰
task_proj, ret_proj, conflict = project_if_conflict(
    task_grads, retention_grads, conflict_threshold=-0.1
)
if conflict:
    print("Conflict detected and resolved!")

# 5. åˆå¹¶æ¢¯åº¦
final_grads = merge_grads(task_proj, ret_proj, lambda_weight=1.5)

# 6. è®¾ç½®æ¢¯åº¦
optimizer.zero_grad()
for param, grad in zip(model.parameters(), final_grads):
    if grad is not None:
        param.grad = grad

# 7. ä¼˜åŒ–å™¨æ›´æ–°
optimizer.step()
```

### è‡ªå®šä¹‰ Îµ è°ƒåº¦

```python
from lerobot.craft.primal_dual import epsilon_schedule

# çº¿æ€§é€€ç«
for step in range(10000):
    eps = epsilon_schedule(
        step, epsilon_start=1.0, epsilon_end=0.1,
        total_steps=10000, schedule_type="linear"
    )
    print(f"Step {step}: epsilon = {eps:.4f}")
```

---

## ç±»å‹å®šä¹‰

```python
from typing import TypedDict

class AnchorBatch(TypedDict):
    """é”šç‚¹æ•°æ®æ‰¹æ¬¡ç±»å‹"""
    pixel_values: torch.Tensor  # [B, C, H, W]
    input_ids: torch.Tensor     # [B, seq_len]
    attention_mask: torch.Tensor  # [B, seq_len]
    target_features: torch.Tensor  # [B, hidden_dim]
    meta: dict

class CraftMetrics(TypedDict):
    """CRaFT è®­ç»ƒæŒ‡æ ‡ç±»å‹"""
    retention_loss: float
    lambda_value: float
    epsilon_value: float
    grad_dot: float
    grad_cos: float
    grad_conflict: bool
```

---

## å¸¸é‡

```python
# é»˜è®¤é…ç½®
DEFAULT_CRAFT_CONFIG = CraftConfig(
    enabled=False,
    retention_mode="hidden",
    initial_lambda=1.0,
    lambda_lr=0.01,
    epsilon_start=1.0,
    epsilon_end=0.1,
    use_grad_projection=True,
    conflict_threshold=-0.1
)

# æ”¯æŒçš„ retention æ¨¡å¼
RETENTION_MODES = ["token_ce", "hidden"]

# æ”¯æŒçš„ pooling ç­–ç•¥
POOLING_STRATEGIES = [
    "mean_image_tokens",
    "mean_masked",
    "last_token",
    "cls_token"
]

# æ”¯æŒçš„æ¢¯åº¦åˆå¹¶æ¨¡å¼
MERGE_MODES = ["weighted", "equal", "task_priority"]
```

---

## å¼‚å¸¸

```python
class CraftConfigError(Exception):
    """CRaFT é…ç½®é”™è¯¯"""
    pass

class AnchorCacheError(Exception):
    """é”šç‚¹æ•°æ®åŠ è½½é”™è¯¯"""
    pass

class GradientSurgeryError(Exception):
    """æ¢¯åº¦æ‰‹æœ¯é”™è¯¯"""
    pass
```

---

## å‚è€ƒ

- [CRaFT è®­ç»ƒæŒ‡å—](craft/CRAFT_TRAINING_GUIDE.md)
- [Hidden Feature Cache](HIDDEN_FEATURE_CACHE_SUMMARY.md)
- [å®éªŒæ“ä½œæŒ‡å—](EXPERIMENT_GUIDE.md)

---

**æœ€åæ›´æ–°**: 2026-02-17

